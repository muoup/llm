Loading tokenizer from: .models/tinystories.tok
Loading existing model from: .models/tinystories-1m.lm
Instantiating model with vocab size 4096 and dimensions 128.
Loading Embedding Layer
Loading Model Layer
Loading Model Layer
Loading Logit Layer
Successfully loaded model. Parameter count: 1250432
Starting training process...
Loading dataset from: datasets/tinystories/full.txt
Dataset loaded. Type: row-based. Iterating over rows...
[DEBUG]   Embedding Layer Forward:
[DEBUG]     output norm pre pos encoding: 8.546615
[DEBUG]     output norm: 134.460098
[DEBUG]   Attention Head 0 Forward:
[DEBUG]     wq norm: 18.218716
[DEBUG]     wk norm: 18.218828
[DEBUG]     wv norm: 18.277273
[DEBUG]     q norm: 267.121185
[DEBUG]     k norm: 264.952179
[DEBUG]     v norm: 304.968872
[DEBUG]     scores norm: 3.695016
[DEBUG]     weighted_sum norm: 295.314911
[DEBUG]   Attention Layer Forward:
[DEBUG]     input norm: 199.021027
[DEBUG]     concatenated_heads norm: 295.314941
[DEBUG]     final_output norm: 3648.996338
[DEBUG]   LayerNorm Forward:
[DEBUG]     input norm: 134.460098
[DEBUG]     normalized norm: 199.021088
[DEBUG]     mean norm: 4.538338
[DEBUG]     inv_variance norm: 26.932438
[DEBUG]     post_residual_connection norm: 3701.310791
[DEBUG]   FF Layer Forward:
[DEBUG]     input norm: 195.269211
[DEBUG]     activation_input norm: 622.641968
[DEBUG]     activation_output norm: 196.250992
[DEBUG]     final_output norm: 1054.226685
[DEBUG]   LayerNorm Forward:
[DEBUG]     input norm: 3701.310059
[DEBUG]     normalized norm: 195.269211
[DEBUG]     mean norm: 326.135010
[DEBUG]     inv_variance norm: 11.490920
[DEBUG]     post_residual_connection norm: 2713.407227
[DEBUG]   Logit Layer Forward:
[DEBUG]     input norm: 2713.406982
[DEBUG]     logits norm pre-softmax: 72517.007812
[DEBUG]     logits norm post-softmax: 1.729554
[DEBUG]   Logit Layer Gradients:
[DEBUG]     logit_weight_gradient norm: 81.550995
[DEBUG]     logit_bias_gradient norm: 28.600565
[DEBUG]   FF Layer Gradients:
[DEBUG]     w1_gradient norm: 24.568235
[DEBUG]     b1_gradient norm: 2.181904
[DEBUG]     w2_gradient norm: 57.014362
[DEBUG]     b2_gradient norm: 7.458461
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 3701.310547
[DEBUG]     normalized_input norm: 195.269226
[DEBUG]     mean norm: 326.135010
[DEBUG]     inv_variance norm: 11.490920
[DEBUG]     grad_normalized norm: 12.327168
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 12.911397
[DEBUG]     grad_beta norm: 13.211281
[DEBUG]     grad_input norm: 0.968492
[DEBUG]   Attention Layer Backpropagation:
[DEBUG]     post_layer_gradient norm: 4.311560
[DEBUG]   Attention Head 0 Gradients:
[DEBUG]     scores_gradient norm: 553.278015
[DEBUG]     raw_scores_gradient norm: 3.8606021553e-02
[DEBUG]     wq_gradient norm: 7.209184
[DEBUG]     wk_gradient norm: 9.514937
[DEBUG]     wv_gradient norm: 47.235462
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 134.460098
[DEBUG]     normalized_input norm: 199.021057
[DEBUG]     mean norm: 4.538338
[DEBUG]     inv_variance norm: 26.932438
[DEBUG]     grad_normalized norm: 10.138763
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 25.695724
[DEBUG]     grad_beta norm: 28.508179
[DEBUG]     grad_input norm: 2.489594
[DEBUG]   Embedding Layer Gradients:
[DEBUG]     embedding_gradient norm: 6.339301
Row 0 / 1 processed. Loss: 5.44, Rolling Avg Loss: 0.05
Training complete. Saving model to: .models/tinystories-1m.lm
