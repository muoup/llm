Loading tokenizer from: .models/tinystories.tok
Creating and randomizing new model.
New model created. Parameter count: 1250688
Starting training process...
Loading dataset from: datasets/tinystories/full.txt
Dataset loaded. Type: row-based. Iterating over rows...
[DEBUG]   Embedding Layer Forward:
[DEBUG]     output norm pre pos encoding: 47.582493
[DEBUG]     output norm: 145.088715
[DEBUG]   Attention Head 0 Forward:
[DEBUG]     wq norm: 15.918497
[DEBUG]     wk norm: 15.918497
[DEBUG]     wv norm: 15.918496
[DEBUG]     q norm: 284.357117
[DEBUG]     k norm: 284.357117
[DEBUG]     v norm: 284.357117
[DEBUG]     scores norm: 14.941442
[DEBUG]     weighted_sum norm: 280.678345
[DEBUG]   Attention Layer Forward:
[DEBUG]     input norm: 193.987579
[DEBUG]     concatenated_heads norm: 280.678375
[DEBUG]     final_output norm: 382.861938
[DEBUG]   LayerNorm Forward:
[DEBUG]     input norm: 145.088699
[DEBUG]     normalized norm: 193.987579
[DEBUG]     mean norm: 5.187061
[DEBUG]     inv_variance norm: 25.222609
[DEBUG]     post_residual_connection norm: 400.533356
[DEBUG]   FF Layer Forward:
[DEBUG]     input norm: 193.989456
[DEBUG]     activation_input norm: 787.118164
[DEBUG]     activation_output norm: 564.469910
[DEBUG]     final_output norm: 482.435486
[DEBUG]   LayerNorm Forward:
[DEBUG]     input norm: 400.533295
[DEBUG]     normalized norm: 193.989456
[DEBUG]     mean norm: 5.080314
[DEBUG]     inv_variance norm: 8.501101
[DEBUG]     post_residual_connection norm: 608.770264
[DEBUG]   Logit Layer Forward:
[DEBUG]     input norm: 193.989609
[DEBUG]     logits norm pre-softmax: 273.554169
[DEBUG]     logits norm post-softmax: 0.276347
[DEBUG]   Logit Layer Gradients:
[DEBUG]     logit_weight_gradient norm: 67.493507
[DEBUG]     logit_bias_gradient norm: 39.847065
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 608.770386
[DEBUG]     normalized_input norm: 193.989594
[DEBUG]     mean norm: 4.065843
[DEBUG]     inv_variance norm: 5.506473
[DEBUG]     grad_normalized norm: 4.239150
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 7.757484
[DEBUG]     grad_beta norm: 10.080378
[DEBUG]     grad_input norm: 1.348815
[DEBUG]   FF Layer Gradients:
[DEBUG]     w1_gradient norm: 93.720879
[DEBUG]     b1_gradient norm: 10.984551
[DEBUG]     w2_gradient norm: 127.312752
[DEBUG]     b2_gradient norm: 13.244974
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 400.533356
[DEBUG]     normalized_input norm: 193.989456
[DEBUG]     mean norm: 5.080314
[DEBUG]     inv_variance norm: 8.501101
[DEBUG]     grad_normalized norm: 7.875054
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 11.974961
[DEBUG]     grad_beta norm: 14.378294
[DEBUG]     grad_input norm: 3.880733
[DEBUG]   Attention Layer Backpropagation:
[DEBUG]     post_layer_gradient norm: 6.810408
[DEBUG]   Attention Head 0 Gradients:
[DEBUG]     scores_gradient norm: 219.994064
[DEBUG]     raw_scores_gradient norm: 8.1432513893e-02
[DEBUG]     wq_gradient norm: 7.081378
[DEBUG]     wk_gradient norm: 7.350599
[DEBUG]     wv_gradient norm: 149.920105
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 145.088715
[DEBUG]     normalized_input norm: 193.987579
[DEBUG]     mean norm: 5.187061
[DEBUG]     inv_variance norm: 25.222609
[DEBUG]     grad_normalized norm: 12.104288
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 17.840204
[DEBUG]     grad_beta norm: 23.635466
[DEBUG]     grad_input norm: 17.439127
[DEBUG]   Embedding Layer Gradients:
[DEBUG]     embedding_gradient norm: 297.396881
Row 0 / 1 processed. Loss: 8.34, Rolling Avg Loss: 0.08
Training complete. Saving model to: .models/tinystories-1m.lm
