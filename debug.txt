Loading tokenizer from: .models/tinystories.tok
Creating and randomizing new model.
New model created. Parameter count: 1448192
Starting training process...
Loading dataset from: datasets/tinystories/full.txt
Dataset loaded. Type: row-based. Iterating over rows...
[DEBUG]   Embedding Layer Forward:
[DEBUG]     output norm pre pos encoding: 6.312017
[DEBUG]     output norm: 1825.094604
[DEBUG]   Attention Head 0 Forward:
[DEBUG]     wq norm: 0.327694
[DEBUG]     wk norm: 0.327694
[DEBUG]     wv norm: 0.327694
[DEBUG]     q norm: 653.106873
[DEBUG]     k norm: 653.106873
[DEBUG]     v norm: 653.106934
[DEBUG]     scores norm: 0.058380
[DEBUG]     weighted_sum norm: 651.997681
[DEBUG]   Attention Head 1 Forward:
[DEBUG]     wq norm: 0.327694
[DEBUG]     wk norm: 0.327694
[DEBUG]     wv norm: 0.327694
[DEBUG]     q norm: 653.106873
[DEBUG]     k norm: 653.106812
[DEBUG]     v norm: 653.106873
[DEBUG]     scores norm: 0.058380
[DEBUG]     weighted_sum norm: 651.997742
[DEBUG]   Attention Layer Forward:
[DEBUG]     input norm: 969.427002
[DEBUG]     concatenated_heads norm: 922.063965
[DEBUG]     final_output norm: 1790.723633
[DEBUG]   LayerNorm Forward:
[DEBUG]     input norm: 1825.094604
[DEBUG]     normalized norm: 969.427002
[DEBUG]     mean norm: 20.610004
[DEBUG]     inv_variance norm: 726.542969
[DEBUG]     pre_residual_connection norm: 1790.723633
[DEBUG]     post_residual_connection norm: 2608.655273
[DEBUG]   FF Layer Forward:
[DEBUG]     input norm: 977.117798
[DEBUG]     activation_input norm: 125.500458
[DEBUG]     activation_output norm: 125.500443
[DEBUG]     final_output norm: 775.505737
[DEBUG]   LayerNorm Forward:
[DEBUG]     input norm: 2608.655518
[DEBUG]     normalized norm: 977.117798
[DEBUG]     mean norm: 66.709824
[DEBUG]     inv_variance norm: 719.204895
[DEBUG]     pre_residual_connection norm: 775.505798
[DEBUG]     post_residual_connection norm: 5057.352539
[DEBUG]   Attention Head 0 Forward:
[DEBUG]     wq norm: 0.327694
[DEBUG]     wk norm: 0.327694
[DEBUG]     wv norm: 0.327694
[DEBUG]     q norm: 569.265015
[DEBUG]     k norm: 569.265015
[DEBUG]     v norm: 569.264954
[DEBUG]     scores norm: 0.058389
[DEBUG]     weighted_sum norm: 568.142273
[DEBUG]   Attention Head 1 Forward:
[DEBUG]     wq norm: 0.327694
[DEBUG]     wk norm: 0.327694
[DEBUG]     wv norm: 0.327694
[DEBUG]     q norm: 569.264893
[DEBUG]     k norm: 569.264954
[DEBUG]     v norm: 569.264893
[DEBUG]     scores norm: 0.058389
[DEBUG]     weighted_sum norm: 568.142151
[DEBUG]   Attention Layer Forward:
[DEBUG]     input norm: 864.067139
[DEBUG]     concatenated_heads norm: 803.474487
[DEBUG]     final_output norm: 1561.857422
[DEBUG]   LayerNorm Forward:
[DEBUG]     input norm: 5057.352051
[DEBUG]     normalized norm: 864.067261
[DEBUG]     mean norm: 259.666168
[DEBUG]     inv_variance norm: 726.177734
[DEBUG]     pre_residual_connection norm: 1561.857056
[DEBUG]     post_residual_connection norm: 11061.153320
[DEBUG]   FF Layer Forward:
[DEBUG]     input norm: 871.686218
[DEBUG]     activation_input norm: 105.893532
[DEBUG]     activation_output norm: 105.893524
[DEBUG]     final_output norm: 673.091187
[DEBUG]   LayerNorm Forward:
[DEBUG]     input norm: 11061.153320
[DEBUG]     normalized norm: 871.686218
[DEBUG]     mean norm: 772.513123
[DEBUG]     inv_variance norm: 714.194031
[DEBUG]     pre_residual_connection norm: 673.091064
[DEBUG]     post_residual_connection norm: 16449.591797
[DEBUG]   Logit Layer Forward:
[DEBUG]     input norm: 16449.593750
[DEBUG]     logits norm pre-softmax: 1846911.250000
[DEBUG]     logits norm post-softmax: 0.002422
[DEBUG]   Logit Layer Gradients:
[DEBUG]     logit_weight_gradient norm: 2869.778809
[DEBUG]     logit_bias_gradient norm: 382.140625
[DEBUG]   FF Layer Gradients:
[DEBUG]     w1_gradient norm: 19.034782
[DEBUG]     b1_gradient norm: 1.244738
[DEBUG]     w2_gradient norm: 113.944016
[DEBUG]     b2_gradient norm: 16.661381
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 11061.153320
[DEBUG]     normalized_input norm: 871.686096
[DEBUG]     mean norm: 772.513123
[DEBUG]     inv_variance norm: 714.194031
[DEBUG]     grad_normalized norm: 0.064203
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 2.636662
[DEBUG]     grad_beta norm: 4.651201
[DEBUG]     grad_input norm: 0.004406
[DEBUG]   Attention Layer Backpropagation:
[DEBUG]     layer_output norm: 11061.153320
[DEBUG]     post_layer_gradient norm: 1.588801
[DEBUG]   Attention Head 0 Gradients:
[DEBUG]     scores_gradient norm: 47.781044
[DEBUG]     raw_scores_gradient norm: 0.000000
[DEBUG]     wq_gradient norm: 0.000219
[DEBUG]     wk_gradient norm: 0.002710
[DEBUG]     wv_gradient norm: 23.514482
[DEBUG]   Attention Head 1 Gradients:
[DEBUG]     scores_gradient norm: 50.207333
[DEBUG]     raw_scores_gradient norm: 0.000000
[DEBUG]     wq_gradient norm: 0.000256
[DEBUG]     wk_gradient norm: 0.003191
[DEBUG]     wv_gradient norm: 28.060129
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 5057.352539
[DEBUG]     normalized_input norm: 864.067139
[DEBUG]     mean norm: 259.666168
[DEBUG]     inv_variance norm: 726.177734
[DEBUG]     grad_normalized norm: 0.031371
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 4.875205
[DEBUG]     grad_beta norm: 9.146310
[DEBUG]     grad_input norm: 0.002177
[DEBUG]   FF Layer Gradients:
[DEBUG]     w1_gradient norm: 21.733919
[DEBUG]     b1_gradient norm: 1.244470
[DEBUG]     w2_gradient norm: 153.069443
[DEBUG]     b2_gradient norm: 18.483265
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 2608.655518
[DEBUG]     normalized_input norm: 977.117798
[DEBUG]     mean norm: 66.709824
[DEBUG]     inv_variance norm: 719.204895
[DEBUG]     grad_normalized norm: 0.063971
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 2.560414
[DEBUG]     grad_beta norm: 4.621586
[DEBUG]     grad_input norm: 0.004433
[DEBUG]   Attention Layer Backpropagation:
[DEBUG]     layer_output norm: 2608.655273
[DEBUG]     post_layer_gradient norm: 1.604653
[DEBUG]   Attention Head 0 Gradients:
[DEBUG]     scores_gradient norm: 54.587086
[DEBUG]     raw_scores_gradient norm: 0.000000
[DEBUG]     wq_gradient norm: 0.000219
[DEBUG]     wk_gradient norm: 0.002617
[DEBUG]     wv_gradient norm: 27.337505
[DEBUG]   Attention Head 1 Gradients:
[DEBUG]     scores_gradient norm: 57.541901
[DEBUG]     raw_scores_gradient norm: 0.000000
[DEBUG]     wq_gradient norm: 0.000264
[DEBUG]     wk_gradient norm: 0.003253
[DEBUG]     wv_gradient norm: 32.654228
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 1825.094604
[DEBUG]     normalized_input norm: 969.427002
[DEBUG]     mean norm: 20.610004
[DEBUG]     inv_variance norm: 726.542969
[DEBUG]     grad_normalized norm: 0.031301
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 4.890304
[DEBUG]     grad_beta norm: 9.122873
[DEBUG]     grad_input norm: 0.002162
[DEBUG]   Embedding Layer Gradients:
[DEBUG]     embedding_gradient norm: 2.836537
Row 0 / 1 processed. Loss: 8.51, Rolling Avg Loss: 0.09
Training complete. Saving model to: .models/tinystories.lm
