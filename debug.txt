Loading tokenizer from: .models/tinystories.tok
Creating and randomizing new model.
New model created. Parameter count: 1448192
Starting training process...
Loading dataset from: datasets/tinystories/full.txt
Dataset loaded. Type: row-based. Iterating over rows...
[DEBUG]   Embedding Layer Forward:
[DEBUG]     output norm pre pos encoding: 6.253304
[DEBUG]     output norm: 1824.758911
[DEBUG]   Attention Head 0 Forward:
[DEBUG]     wq norm: 0.327016
[DEBUG]     wk norm: 0.327016
[DEBUG]     wv norm: 0.327016
[DEBUG]     q norm: 687.362305
[DEBUG]     k norm: 687.362305
[DEBUG]     v norm: 687.362244
[DEBUG]     scores norm: 0.059210
[DEBUG]     weighted_sum norm: 688.568237
[DEBUG]   Attention Head 1 Forward:
[DEBUG]     wq norm: 0.327016
[DEBUG]     wk norm: 0.327016
[DEBUG]     wv norm: 0.327016
[DEBUG]     q norm: 687.362244
[DEBUG]     k norm: 687.362305
[DEBUG]     v norm: 687.362305
[DEBUG]     scores norm: 0.059210
[DEBUG]     weighted_sum norm: 688.568298
[DEBUG]   Attention Layer Forward:
[DEBUG]     input norm: 1026.088867
[DEBUG]     concatenated_heads norm: 973.782532
[DEBUG]     final_output norm: 1924.520142
[DEBUG]   LayerNorm Forward:
[DEBUG]     input norm: 1824.759033
[DEBUG]     normalized norm: 1026.088623
[DEBUG]     mean norm: 20.651831
[DEBUG]     inv_variance norm: 726.794312
[DEBUG]     pre_residual_connection norm: 1924.520264
[DEBUG]     post_residual_connection norm: 2685.681396
[DEBUG]   FF Layer Forward:
[DEBUG]     input norm: 1010.667053
[DEBUG]     activation_input norm: 136.547806
[DEBUG]     activation_output norm: 136.547775
[DEBUG]     final_output norm: 847.339355
[DEBUG]   LayerNorm Forward:
[DEBUG]     input norm: 2685.681152
[DEBUG]     normalized norm: 1010.666992
[DEBUG]     mean norm: 74.232506
[DEBUG]     inv_variance norm: 736.356567
[DEBUG]     pre_residual_connection norm: 847.339355
[DEBUG]     post_residual_connection norm: 5326.877441
[DEBUG]   Attention Head 0 Forward:
[DEBUG]     wq norm: 0.327016
[DEBUG]     wk norm: 0.327016
[DEBUG]     wv norm: 0.327016
[DEBUG]     q norm: 603.357544
[DEBUG]     k norm: 603.357544
[DEBUG]     v norm: 603.357544
[DEBUG]     scores norm: 0.059192
[DEBUG]     weighted_sum norm: 604.411560
[DEBUG]   Attention Head 1 Forward:
[DEBUG]     wq norm: 0.327016
[DEBUG]     wk norm: 0.327016
[DEBUG]     wv norm: 0.327016
[DEBUG]     q norm: 603.357544
[DEBUG]     k norm: 603.357544
[DEBUG]     v norm: 603.357544
[DEBUG]     scores norm: 0.059192
[DEBUG]     weighted_sum norm: 604.411560
[DEBUG]   Attention Layer Forward:
[DEBUG]     input norm: 883.975586
[DEBUG]     concatenated_heads norm: 854.766968
[DEBUG]     final_output norm: 1689.465210
[DEBUG]   LayerNorm Forward:
[DEBUG]     input norm: 5326.876465
[DEBUG]     normalized norm: 883.975769
[DEBUG]     mean norm: 288.629822
[DEBUG]     inv_variance norm: 746.294006
[DEBUG]     pre_residual_connection norm: 1689.465332
[DEBUG]     post_residual_connection norm: 11787.657227
[DEBUG]   FF Layer Forward:
[DEBUG]     input norm: 873.624939
[DEBUG]     activation_input norm: 116.051407
[DEBUG]     activation_output norm: 116.051430
[DEBUG]     final_output norm: 739.150879
[DEBUG]   LayerNorm Forward:
[DEBUG]     input norm: 11787.658203
[DEBUG]     normalized norm: 873.624939
[DEBUG]     mean norm: 848.901794
[DEBUG]     inv_variance norm: 747.630676
[DEBUG]     pre_residual_connection norm: 739.150940
[DEBUG]     post_residual_connection norm: 17703.855469
[DEBUG]   Logit Layer Forward:
[DEBUG]     input norm: 17703.855469
[DEBUG]     logits norm pre-softmax: 2041616.375000
[DEBUG]     logits norm post-softmax: 0.002619
[DEBUG]   Logit Layer Gradients:
[DEBUG]     logit_weight_gradient norm: 3117.595459
[DEBUG]     logit_bias_gradient norm: 379.387299
[DEBUG]   FF Layer Gradients:
[DEBUG]     w1_gradient norm: 4.899896
[DEBUG]     b1_gradient norm: 0.317344
[DEBUG]     w2_gradient norm: 103.224419
[DEBUG]     b2_gradient norm: 13.491074
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 11787.658203
[DEBUG]     normalized_input norm: 873.624878
[DEBUG]     mean norm: 848.901794
[DEBUG]     inv_variance norm: 747.630676
[DEBUG]     grad_normalized norm: 0.071461
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 0.575294
[DEBUG]     grad_beta norm: 1.047902
[DEBUG]     grad_input norm: 0.005128
[DEBUG]   Attention Layer Backpropagation:
[DEBUG]     layer_output norm: 11787.657227
[DEBUG]     post_layer_gradient norm: 1.600254
[DEBUG]   Attention Head 0 Gradients:
[DEBUG]     scores_gradient norm: 56.821960
[DEBUG]     raw_scores_gradient norm: 0.000000
[DEBUG]     wq_gradient norm: 0.000041
[DEBUG]     wk_gradient norm: 0.000692
[DEBUG]     wv_gradient norm: 6.273195
[DEBUG]   Attention Head 1 Gradients:
[DEBUG]     scores_gradient norm: 53.982841
[DEBUG]     raw_scores_gradient norm: 0.000000
[DEBUG]     wq_gradient norm: 0.000045
[DEBUG]     wk_gradient norm: 0.000792
[DEBUG]     wv_gradient norm: 6.832440
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 5326.876953
[DEBUG]     normalized_input norm: 883.975708
[DEBUG]     mean norm: 288.629822
[DEBUG]     inv_variance norm: 746.294006
[DEBUG]     grad_normalized norm: 0.006742
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 0.947887
[DEBUG]     grad_beta norm: 1.943099
[DEBUG]     grad_input norm: 0.000488
[DEBUG]   FF Layer Gradients:
[DEBUG]     w1_gradient norm: 5.747150
[DEBUG]     b1_gradient norm: 0.317918
[DEBUG]     w2_gradient norm: 129.376419
[DEBUG]     b2_gradient norm: 14.108127
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 2685.680664
[DEBUG]     normalized_input norm: 1010.666992
[DEBUG]     mean norm: 74.232506
[DEBUG]     inv_variance norm: 736.356567
[DEBUG]     grad_normalized norm: 0.071425
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 0.575820
[DEBUG]     grad_beta norm: 1.046859
[DEBUG]     grad_input norm: 0.004958
[DEBUG]   Attention Layer Backpropagation:
[DEBUG]     layer_output norm: 2685.681152
[DEBUG]     post_layer_gradient norm: 1.612634
[DEBUG]   Attention Head 0 Gradients:
[DEBUG]     scores_gradient norm: 63.999332
[DEBUG]     raw_scores_gradient norm: 0.000000
[DEBUG]     wq_gradient norm: 0.000037
[DEBUG]     wk_gradient norm: 0.000588
[DEBUG]     wv_gradient norm: 7.181442
[DEBUG]   Attention Head 1 Gradients:
[DEBUG]     scores_gradient norm: 61.185135
[DEBUG]     raw_scores_gradient norm: 0.000000
[DEBUG]     wq_gradient norm: 0.000041
[DEBUG]     wk_gradient norm: 0.000700
[DEBUG]     wv_gradient norm: 7.973351
[DEBUG]   LayerNorm Inputs: 
[DEBUG]     layer_input norm: 1824.759033
[DEBUG]     normalized_input norm: 1026.088745
[DEBUG]     mean norm: 20.651831
[DEBUG]     inv_variance norm: 726.794312
[DEBUG]     grad_normalized norm: 0.006619
[DEBUG]   LayerNorm Layer Gradients:
[DEBUG]     grad_gamma norm: 0.966862
[DEBUG]     grad_beta norm: 1.906829
[DEBUG]     grad_input norm: 0.000458
[DEBUG]   Embedding Layer Gradients:
[DEBUG]     embedding_gradient norm: 2.626809
Row 0 / 1 processed. Loss: 8.34, Rolling Avg Loss: 0.08
Training complete. Saving model to: .models/tinystories.lm
